////////////////////////////////////////////////////////////////////////////////
// Copyright (c) 2014-2022, Lawrence Livermore National Security, LLC.
// Produced at the Lawrence Livermore National Laboratory.
// Written by the LBANN Research Team (B. Van Essen, et al.) listed in
// the CONTRIBUTORS file. <lbann-dev@llnl.gov>
//
// LLNL-CODE-697807.
// All rights reserved.
//
// This file is part of LBANN: Livermore Big Artificial Neural Network
// Toolkit. For details, see http://software.llnl.gov/LBANN or
// https://github.com/LLNL/LBANN.
//
// Licensed under the Apache License, Version 2.0 (the "Licensee"); you
// may not use this file except in compliance with the License.  You may
// obtain a copy of the License at:
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the license.
////////////////////////////////////////////////////////////////////////////////

syntax = "proto3";

package lbann_data;

import "google/protobuf/wrappers.proto";

import "datatype.proto";
import "operators.proto";

// Lives here because it's used by both Convolution and
// Deconvolution.
enum ConvTensorOpsMode {
  // Use the global default.
  DEFAULT_TENSOR_OPS = 0;
  // Explicitly disable tensor ops.
  NO_TENSOR_OPS = 1;
  // Use tensor ops -- allows conversion FP32->FP16
  USE_TENSOR_OPS = 2;
}

/** @brief Neural network tensor operation
 *
 *  Layers in a neural network are arranged as a directed acyclic
 *  graph. They take one input tensor from each parent layer and send
 *  an output tensor to each child layer. Some layers may recieve
 *  tensors from weights objects (trainable parameters).
 *
 *  LBANN performs implicit mini-batching. If the user specifies a
 *  layer to handle 3D image data (in channel-height-width format), it
 *  is stored internally as a 4D tensor (in NCHW format). This scheme
 *  implies that computation is mostly independent between mini-batch
 *  samples (with a few exceptions like batchnorm).
 *
 *  The default value for fields in protobuf messages is zero-like
 *  (false for bool, empty string for string). Thus, all defaults are
 *  zero-like unless otherwise stated.
 */
message Layer {

  // ===========================================
  // Basic layer options
  // ===========================================

  /** @brief Unique identifier for layer
   */
  string name = 1;

  /** @brief Parent layers
   *  @details List of layer names
   */
  repeated string parents = 2;
  /** @brief Child layers
   *  @details List of layer names
   */
  repeated string children = 3;
  /** @brief Weights objects
   *
   *  List of weights names. Weights are typically used as trainable
   *  parameters.
   */
  repeated string weights = 4;

  /** @brief Data tensor device
   *
   *  If LBANN has been built with GPU support, default is GPU.
   *  Otherwise, CPU.
   *
   *  Options: CPU or GPU
   */
  string device_allocation = 5;
  /** @brief Data tensors datatype */
  DataType datatype = 6;

  // ===========================================
  // Advanced options
  // ===========================================

  /** @brief Hint layer for configuration
   *
   *  Advanced option for configuring certain layers. Typically used
   *  to specify that a layer has the same output dimensions as
   *  another.
   */
  string hint_layer = 10;
  /** @brief Data tensor layout
   *  @details Options: data_parallel (default) or model_parallel
   */
  string data_layout = 11;
  /** @brief Configuration for advanced parallelization strategies */
  ParallelStrategy parallel_strategy = 12;

  // ===========================================
  // Deprecated options
  // ===========================================

  /// Deprecated
  bool num_neurons_from_data_reader = 400;
  /// Deprecated
  bool freeze = 401;
  /// Deprecated and unused
  repeated WeightsData weights_data = 402;

  // ===========================================
  // Concrete layer types
  // ===========================================
  // Note: This is a somewhat hacky implementation of class
  // inheritance.

  oneof layer_type {

    // Input layer
    Input input = 20;

    // Operator layer
    OperatorLayer operator_layer = 21; // Name chosen to avoid collision

    // Transform layers
    Reshape reshape = 50;
    Pooling pooling = 51;
    Concatenation concatenation = 52;
    Slice slice = 53;
    Split split = 54;
    Sum sum = 55;
    Cross_Grid_Sum_Slice cross_grid_sum_slice = 56;
    Cross_Grid_Sum cross_grid_sum = 57;
    WeightedSum weighted_sum = 58;
    Unpooling unpooling = 59;
    Hadamard hadamard = 60;
    Constant constant = 61;
    Reduction reduction = 62;
    Evaluation evaluation = 63;
    Gaussian gaussian = 64;
    Bernoulli bernoulli = 65;
    Uniform uniform = 66;
    Crop crop = 67;
    Dummy dummy = 68;
    StopGradient stop_gradient = 69;
    InTopK in_top_k = 70;
    Sort sort = 71;
    WeightsLayer weights_layer = 72;
    Tessellate tessellate = 73;
    Scatter scatter = 74;
    Gather gather = 75;
    BatchwiseReduceSum batchwise_reduce_sum = 76;
    TensorPermute permute = 77;
    CategoricalRandom categorical_random = 406; // Deprecated
    DiscreteRandom discrete_random = 407; // Deprecated

    // Learning layers
    FullyConnected fully_connected = 100;
    Convolution convolution = 101;
    Deconvolution deconvolution = 102;
    Embedding embedding = 103;
    ChannelwiseScaleBias channelwise_scale_bias = 104;
    EntrywiseScaleBias entrywise_scale_bias = 105;
    ChannelwiseFullyConnected channelwise_fully_connected = 106;
    GRU gru = 107;

    // Loss layers
    CrossEntropy cross_entropy = 120;
    MeanSquaredError mean_squared_error = 121;
    MeanAbsoluteError mean_absolute_error = 122;
    CategoricalAccuracy categorical_accuracy = 123;
    TopKCategoricalAccuracy top_k_categorical_accuracy = 124;
    L2Norm2 l2_norm2 = 125;
    L1Norm l1_norm = 126;

    // Math layers
    MatMul matmul = 140;
    DFTAbs dft_abs = 141;

    // Regularization layers
    BatchNormalization batch_normalization = 160;
    LocalResponseNormalization local_response_normalization = 161;
    Dropout dropout = 162;
    SeluDropout selu_dropout = 163;
    EntrywiseBatchNormalization entrywise_batch_normalization = 164;
    LayerNorm layer_norm = 165;
    InstanceNorm instance_norm = 166;

    // Activation layers
    Elu elu = 180;
    Identity identity = 181;
    LeakyRelu leaky_relu = 182;
    LogSoftmax log_softmax = 183;
    Relu relu = 184;
    Softmax softmax = 185;

    // Image layers
    BilinearResize bilinear_resize = 200;
    Rotation rotation = 201;
    CompositeImageTransformation composite_image_transformation = 202;
    Cutout cutout = 203;

    // Miscellaneous layers
    Covariance covariance = 300;
    Variance variance = 301;
    ChannelwiseMean channelwise_mean = 302;
    MiniBatchIndex mini_batch_index = 303;
    MiniBatchSize mini_batch_size = 304;
    Argmax argmax = 305;
    Argmin argmin = 306;
    OneHot one_hot = 307;
    ChannelwiseSoftmax channelwise_softmax = 309;
    DistEmbedding dist_embedding = 311;
    UniformHash uniform_hash = 312;
    RowwiseWeightsNorms rowwise_weights_norms = 313;

  }

  // ---------------------------
  // Operator layers
  // ---------------------------

  /** @brief Layer composed of one or more operator objects
   *
   *  Operators are applied sequentially.
   */
  message OperatorLayer {
    repeated Operator ops = 1;
  }

  // ---------------------------
  // Math layers
  // ---------------------------

  /** @brief Absolute value of discrete Fourier transform
   *
   *  One-, two-, or three-dimensional data is allowed.
   *
   *  The implementation is meant to be as flexible as possible. We
   *  use FFTW for the CPU implementation; whichever types your
   *  implementation of FFTW supports will be supported in this layer
   *  at runtime. The GPU implementation uses cuFFT on NVIDIA GPUs and
   *  will support float and double at runtime (assuming CUDA support
   *  is enabled). A future implementation will support rocFFT for AMD
   *  GPUs.
   *
   *  Currently, LBANN only supports outputting the same type that is
   *  used as input. As such, in forward propagation, this will do a
   *  DFT and then compute the absolute value of the output
   *  implicitly. The intention is to support immediate customer need
   *  now; we will generalize this as LBANN learns to support
   *  different input/output data types.
   */
  message DFTAbs {}

  /** @brief Matrix multiplication.
   *
   *  Performs matrix product of two 2D input tensors. If the input
   *  tensors are 3D, then matrix products are computed independently
   *  over the first dimension, in a similar manner as NumPy's matmul
   *  function.
   */
  message MatMul {
    /// Whether to transpose matrices from first input tensor
    bool transpose_a = 1;
    /// Whether to transpose matrices from second input tensor
    bool transpose_b = 2;
  }

  // ---------------------------
  // Activation layers
  // ---------------------------

  /** @brief Exponential linear unit
   *
   *  @f[
   *    \text{ELU}(x; \alpha) =
   *      \begin{cases}
   *        x                & x > 0 \\
   *        \alpha (e^x - 1) & x \leq 0
   *      \end{cases}
   *  @f]
   *  @f$\alpha@f$ should be non-negative. See:
   *
   *  Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter.
   *  "Fast and accurate deep network learning by exponential linear
   *  units (ELUs)." arXiv preprint arXiv:1511.07289 (2015).
   */
  message Elu {
    /// Default: 1. Should be >=0.
    double alpha = 1;
  }
  /** @brief Output the input tensor
   *
   *  This layer is very cheap since it just involves setting up
   *  tensor views.
   */
  message Identity {}

  /**
   *  @f[
   *    \text{LeakyReLU}(x; \alpha) =
   *      \begin{cases}
   *        x        & x > 0 \\
   *        \alpha x & x \leq 0
   *      \end{cases}
   *  @f]
   *  See:
   *
   *  Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. "Rectifier
   *  nonlinearities improve neural network acoustic models." In
   *  Proc. ICML, vol. 30, no. 1, p. 3. 2013.
   */
  message LeakyRelu {
    /// Default: 0.01
    double negative_slope = 1;
  }

  /** @brief Logarithm of softmax function
   *
   *  @f[ \log \text{softmax}(x)_i = x_i - \log \sum_j e^{x_j} @f]
   */
  message LogSoftmax {}

  /** @brief Rectified linear unit
   *
   *  \f[ ReLU(x) = \text{max}(x, 0) \f]
   */
  message Relu {}

  /**
   *  @f[ \text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}} @f]
   */
  message Softmax {
    /// Options: instance (default), channel
    string softmax_mode = 1;
  }

  // ---------------------------
  // Loss layers
  // ---------------------------

  /** @brief Cross entropy between probability vectors
   *
   *  Given a predicted distribution @f$y@f$ and ground truth
   *  distribution @f$\hat{y}@f$,
   *  @f[ CE(y,\hat{y}) = - \sum\limits_{i} \hat{y}_i \log y_i @f]
   */
  message CrossEntropy {
    /// Advanced option for distconv
    bool use_labels = 1;
  }
  /**
   *  Given a prediction @f$y@f$ and ground truth @f$\hat{y}@f$,
   *  @f[
   *    MSE(y,\hat{y})
   *      = \frac{1}{n} \sum\limits_{i=1}^{n} (y_i - \hat{y}_i)^2
   *  @f]
   */
  message MeanSquaredError {}
  /**
   *  Given a prediction @f$y@f$ and ground truth @f$\hat{y}@f$,
   *  @f[
   *    MAE(y,\hat{y})
   *      = \frac{1}{n} \sum\limits_{i=1}^{n} | y_i - \hat{y}_i |
   *  @f]
   */
  message MeanAbsoluteError {}
  /** @brief 0-1 loss function
   *
   *  Requires two inputs, which are respectively interpreted as
   *  prediction scores and as a one-hot label vector. The output is
   *  one if the top entries in both inputs are in the same position
   *  and is otherwise zero. Ties are broken in favor of entries with
   *  smaller indices.
   *  This is primarily intended for use as a metric since it is not
   *  differentiable.
   */
  message CategoricalAccuracy {}
  /**
   *  Requires two inputs, which are respectively interpreted as
   *  prediction scores and as a one-hot label vector. The output is
   *  one if the corresponding label matches one of the top-k
   *  prediction scores and is otherwise zero. Ties in the top-k
   *  prediction scores are broken in favor of entries with smaller
   *  indices.
   */
  message TopKCategoricalAccuracy {
    int64 k = 1;
  }
  /** @brief Square of L2 vector norm
   *
   *  @f[ \lVert x\rVert_2^2 = \sum\limits_{i} x_i^2 @f]
   */
  message L2Norm2 {}
  /** @brief L1 vector norm
   *
   *  @f[ \lVert x\rVert_1 = \sum\limits_{i} | x_i | @f]
   */
  message L1Norm {}

  // ---------------------------
  // Regularization layers
  // ---------------------------

  /** @brief Channel-wise batch normalization
   *
   *  Each input channel is normalized across the mini-batch to have
   *  zero mean and unit standard deviation. Learned scaling factors
   *  and biases are then applied. This uses the standard approach of
   *  maintaining the running mean and standard deviation (with
   *  exponential decay) for use at test time.
   *
   *  This layer maintains four weights: scales, biases, running
   *  means, and running variances. Each has a size equal to the
   *  number of channels. In order to disable the affine operation,
   *  manually construct weights without optimizers.
   *
   *  See:
   *
   *  Sergey Ioffe and Christian Szegedy. "Batch Normalization:
   *  Accelerating Deep Network Training by Reducing Internal
   *  Covariate Shift." In International Conference on Machine
   *  Learning, pp. 448-456. 2015.
   */
  message BatchNormalization {
    /** @brief Decay factor for running statistics
     *  @details Default: 0.9
     */
    double decay = 1;
    /** @brief Small number for numerical stability
     *  @details Default: 1e-5
     */
    double epsilon = 4;
    /** @brief Size of process group for computing statistics
     *
     *  Default: 1
     *
     *  A group size of 1 implies purely local statistics. A negative
     *  group size indicates global statistics (i.e. statistics over
     *  the entire mini-batch).
     */
    int64 statistics_group_size = 6;

    /// Deprecated and unused
    double scale_init = 2;
    /// Deprecated and unsued
    double bias_init = 3;
    /// Deprecated
    string stats_aggregation = 5;

  }

  /** @brief Entry-wise batch normalization
   *
   *  Each input entry is normalized across the mini-batch to have
   *  zero mean and unit standard deviation. This uses the standard
   *  approach of maintaining the running mean and standard deviation
   *  (with exponential decay) for use at test time.
   *
   *  This layer maintains two weights: running means, and running
   *  variances. Each has a shape identical to the data tensor. It is
   *  common to apply an affine operation after this layer, e.g. with
   *  the entry-wise scale/bias layer.
   *
   *  See:
   *
   *  Sergey Ioffe and Christian Szegedy. "Batch Normalization:
   *  Accelerating Deep Network Training by Reducing Internal
   *  Covariate Shift." In International Conference on Machine
   *  Learning, pp. 448-456. 2015.
   */
  message EntrywiseBatchNormalization {
    /** @brief Decay factor for running statistics
     *  @details Recommendation: 0.9
     */
    double decay = 1;
    /** @brief Small number for numerical stability
     *  @details Recommendation: 1e-5
     */
    double epsilon = 2;
  }

  /** @brief Scaled dropout for use with SELU activations.
   *
   *  A default keep probability of 0.95 is recommended. See:
   *
   *  Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp
   *  Hochreiter. "Self-normalizing neural networks." In Advances in
   *  Neural Information Processing Systems, pp. 971-980. 2017.
   */
  message SeluDropout {
    /// Recommendation: 0.95
    double keep_prob = 2;
    /// Default: 1.6732632423543772848170429916717
    double alpha = 3;
    /// Default: 1.0507009873554804934193349852946
    double scale = 4;
  }

  /**
   *  See:
   *
   *  Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. "ImageNet
   *  classification with deep convolutional neural networks." In
   *  Advances in Neural Information Processing Systems,
   *  pp. 1097-1105. 2012.
   */
  message LocalResponseNormalization {
    int64 window_width = 4;
    double lrn_alpha = 5;
    double lrn_beta = 6;
    double lrn_k = 7;
  }

  /** @brief Probabilistically drop tensor entries
   *
   *  The values multiplied by 1/(keep probability) at training time.
   *  Keep probabilities of 0.5 for fully-connected layers and 0.8 for
   *  input layers are good starting points. See:
   *
   *  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
   *  Sutskever, and Ruslan Salakhutdinov. "Dropout: a simple way to
   *  prevent neural networks from overfitting." The Journal of
   *  Machine Learning Research 15, no. 1 (2014): 1929-1958.
   */
  message Dropout {
    /** @brief Probability of keeping each tensor entry
     *  @details Recommendation: 0.5
     */
    double keep_prob = 2;
  }

  /** @brief Normalize over data samples
   *
   *  Each data sample is normalized to have zero mean and unit
   *  standard deviation. See:
   *
   *  Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer
   *  normalization." arXiv preprint arXiv:1607.06450 (2016).
   *
   *  It is common to apply an affine operation after this layer, e.g.
   *  with the entry-wise scale/bias layer.
   */
  message LayerNorm {
    /** @brief Small number to avoid division by zero.
     *  @details Default: 1e-5
     */
    google.protobuf.DoubleValue epsilon = 1;
  }

  /** @brief Normalize over data channels
   *
   *  Each channel within a data sample is normalized to have zero
   *  mean and unit standard deviation. See:
   *
   *  Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. "Instance
   *  normalization: The missing ingredient for fast stylization."
   *  arXiv preprint arXiv:1607.08022 (2016).
   *
   *  This is equivalent to applying layer normalization independently
   *  to each channel. It is common to apply an affine operation after
   *  this layer, e.g. with the channel-wise scale/bias layer.
   */
  message InstanceNorm {
    /** @brief Small number to avoid division by zero.
     *  @details Default: 1e-5
     */
    google.protobuf.DoubleValue epsilon = 1;
  }

  // ---------------------------
  // I/O layers
  // ---------------------------

  /** @brief Data tensor from data reader */
  message Input {
    /** @brief Data field in data reader
     *
     *  Legacy values are "samples", "labels", "responses".
     */
    string data_field = 1;
  }

  // ---------------------------
  // Transform layers
  // ---------------------------

  /** @brief Reinterpret tensor with new dimensions
   *
   *  The input and output tensors must have the same number of
   *  entries. This layer is very cheap since it just involves setting
   *  up tensor views.
   */
  message Reshape {
    /** @brief Tensor dimensions
     *
     *  List of integers. A single dimension may be
     *  -1, in which case the dimension is inferred.
     */
    repeated int64 dims = 1;
  }

  /** @brief Pooling
   *
   *  Traverses the spatial dimensions of a data tensor with a sliding
   *  window and applies a reduction operation.
   */
  message Pooling {

    /** @brief Pooling operation
     *
     *  Options: max, average, average_no_pad
     */
    string pool_mode = 1;

    /** @brief Number of spatial dimensions
     *
     *  The first data dimension is treated as the channel dimension,
     *  and all others are treated as spatial dimensions (recall that
     *  the mini-batch dimension is implicit).
     */
    int64 num_dims = 2;

    /** @brief Whether to use vector-valued options
     *
     *  If true, then the pooling is configured with @c pool_dims,
     *  @c pool_pads, @c pool_strides. Otherwise, @c pool_dims_i,
     *  @c pool_pads_i, @c pool_strides_i.
     */
    bool has_vectors = 3;

    /** @brief Pooling window dimensions (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled.
     */
    repeated int64 pool_dims = 4;
    /** @brief Pooling padding (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled.
     */
    repeated int64 pool_pads = 5;
    /** @brief Pooling strides (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled.
     */
    repeated int64 pool_strides = 6;

    /** @brief Pooling window dimension (integer-valued)
     *
     *  Used when @c has_vectors is disabled.
     */
    int64 pool_dims_i = 7;
    /** @brief Pooling padding (integer-valued)
     *
     *  Used when @c has_vectors is disabled.
     */
    int64 pool_pads_i = 8;
    /** @brief Pooling stride (integer-valued)
     *
     *  Used when @c has_vectors is disabled.
     */
    int64 pool_strides_i = 9;

  }

  /** @brief Transpose of pooling layer
   *
   *  Requires that a pooling layer is set as the hint layer.
   *
   *  @warning This has not been well maintained and is probably
   *  broken.
   *  @todo GPU support.
   */
  message Unpooling {
    /** @brief Number of spatial dimensions
     *
     *  The first data dimension is treated as the channel dimension,
     *  and all others are treated as spatial dimensions (recall that
     *  the mini-batch dimension is implicit).
     */
    int64 num_dims = 1;
  }

  /** @brief Concatenate tensors along specified dimension
   *
   *  All input tensors must have identical dimensions, except for the
   *  concatenation dimension.
   */
  message Concatenation {
    /// Tensor dimension to concatenate along
    int64 axis = 1;
  }

  /** @brief Slice tensor along specified dimension
   *
   *  The tensor is split along one dimension at user-specified
   *  points, and each child layer recieves one piece.
   */
  message Slice {

    /// Tensor dimension to slice along
    int64 axis = 1;
    /** @brief Positions at which to slice tensor
     *
     *  List of integers. Slice points must be in
     *  ascending order and the number of slice points must be one
     *  greater than the number of child layers.
     */
    repeated int64 slice_points = 2;

    /** @brief Deprecated
     *  @details Hack for jag_conduit_hdf5
     */
    string get_slice_points_from_reader = 3;
  }

  /** @brief Output the input tensor to multiple child layers
   *
   *  Rarely needed by users. This layer is used internally to handle
   *  cases where a layer outputs the same tensor to multiple child
   *  layers. From a usage perspective, there is little difference
   *  from an identity layer.
   *
   *  This is not to be confused with the split operation in NumPy,
   *  PyTorch or TensorFlow. The name refers to splits in the compute
   *  graph.
   */
  message Split {
  }

  /** @brief Add multiple tensors */
  message Sum {
  }

  /** @brief Add tensors over multiple sub-grids and slice
   *
   *  This is experimental functionality for use with sub-grid
   *  parallelism.
   */
  message Cross_Grid_Sum_Slice {
  }
  /** @brief Add tensors over multiple sub-grids
   *
   *  This is experimental functionality for use with sub-grid
   *  parallelism.
   */
  message Cross_Grid_Sum {
  }

  /** @brief Add tensors with scaling factors */
  message WeightedSum {
    /** List of floating-point numbers, one for each
     *  input tensor.
     */
    repeated double scaling_factors = 1;
  }

  /** @brief Entry-wise tensor product */
  message Hadamard {
  }

  /** @brief Output tensor filled with a single value */
  message Constant {
    /** @brief Value of tensor entries */
    double value = 1;
    /** @brief Tensor dimensions
     *  @details List of integers
     */
    repeated int64 num_neurons = 2;
  }

  /** @brief Reduce tensor to scalar */
  message Reduction {
    /** @brief Reduction operation
     *  @details Options: sum (default) or mean
     */
    string mode = 1;
  }

  /** @brief Interface with objective function and metrics
   *
   *  Rarely needed by users. Evaluation layers are automatically
   *  created when needed in the compute graph.
   */
  message Evaluation {
  }

  /** @brief Random tensor with Gaussian/normal distribution */
  message Gaussian {
    /** @brief Distribution mean */
    double mean = 1;
    /** @brief Distribution standard deviation */
    double stdev = 2;
    /** @brief Tensor dimensions
     *  @details List of integers
     */
    repeated int64 neuron_dims = 3;
    /** @brief Only generate random values during training
     *
     *  If true, the tensor is filled with the distribution mean
     *  during evaluation.
     */
    bool training_only = 4;
  }

  /** @brief Random tensor with Bernoulli distribution
   *
   *  Randomness is only applied during training. The tensor is filled
   *  with zeros during evaluation.
   */
  message Bernoulli {
    /** @brief Bernoulli distribution probability */
    double prob = 1;
    /** @brief Tensor dimensions
     *  @details List of integers
     */
    repeated int64 neuron_dims = 2;
  }

  /** @brief Random tensor with uniform distribution */
  message Uniform {
    /** @brief Distribution minimum */
    double min = 1;
    /** @brief Distribution maximum */
    double max = 2;
    /** @brief Tensor dimensions
     *  @details List of integers
     */
    repeated int64 neuron_dims = 3;
    /** @brief Only generate random values during training
     *
     *  If true, the tensor is filled with the distribution mean
     *  during evaluation.
     */
    bool training_only = 4;
  }

  /** @brief Extract crop from tensor at a position
   *
   *  Expects two input tensors: an @f$ N @f$-D data tensor and a 1D
   *  position vector with @f$ N @f$ entries. The position vector
   *  should be normalized so that values are in @f$ [0,1] @f$. For
   *  images in CHW format, a position of (0,0,0) corresponds to the
   *  red-top-left corner and (1,1,1) to the blue-bottom-right corner.
   */
  message Crop {
    /** @brief Crop dimensions
     *  @details List of integers
     */
    repeated int64 dims = 3;
  }

  /** @brief Placeholder layer with no child layers
   *
   *  Rarely needed by users. This layer is used internally to handle
   *  cases where a layer has no child layers.
   */
  message Dummy {
  }

  /** @brief Block error signals during back propagation
   *
   *  The output is identical to the input, but the back propagation
   *  output (i.e. the error signal) is always zero. Compare with the
   *  stop_gradient operation in TensorFlow and Keras. Note that this
   *  means that computed gradients in preceeding layers are not exact
   *  gradients of the objective function.
   */
  message StopGradient {
  }

  /** @brief One-hot vector indicating top-k entries
   *
   *  Output tensor has same dimensions as input tensor. Output
   *  entries corresponding to the top-k input entries are set to one
   *  and the rest to zero. Ties are broken in favor of entries with
   *  smaller indices.
   */
  message InTopK {
    /// Number of non-zeros in one-hot vector
    int64 k = 1;
  }

  /** @brief Sort tensor entries */
  message Sort {
    /// Sort entries in descending order
    bool descending = 1;
  }

  /** @brief Output values from a weights tensor
   *
   *  Interfaces with a @c weights object.
   */
  message WeightsLayer {
    /** @brief Weights tensor dimensions
     *  @details List of integers
     */
    repeated int64 dims = 1;
  }

  /** @brief Repeat a tensor until it matches specified dimensions
   *
   *  The output tensor dimensions do not need to be integer multiples
   *  of the input dimensions. Compare with the NumPy @c tile
   *  function.
   *
   *  As an example, tessellating a @f$ 2 \times 2 @f$ matrix into a
   *  @f$ 3 \times 4 @f$ matrix looks like the following:
   *  @f[
   *    \begin{bmatrix}
   *      1 & 2 \\
   *      3 & 4
   *    \end{bmatrix}
   *    \rightarrow
   *    \begin{bmatrix}
   *      1 & 2 & 1 & 2 \\
   *      3 & 4 & 3 & 4 \\
   *      1 & 2 & 1 & 2
   *    \end{bmatrix}
   *  @f]
   */
  message Tessellate {
    /** @brief Output tensor dimensions
     *  @details List of integers
     */
    repeated int64 dims = 1;
  }

  /** @brief Scatter values to specified tensor indices
    *
    *  Expects two input tensors: an @f$ N @f$-D data tensor and a 1D
    *  index vector. For 1D data:
    *  @f[
    *    y[\text{ind}[i]] = x[i]
    *  @f]
    *  Out-of-range indices are ignored.
    *
    *  For higher-dimensional data, the layer performs a scatter along
    *  one dimension. For example, with 2D data and axis=1,
    *  @f[
    *    y[i,\text{ind}[j]] = x[i,j]
    *  @f]
    *  Currently, only 1D and 2D data is supported.
    *
    *  The size of the index vector must match the size of the data
    *  tensor along the scatter dimension.
    *
    *  @todo Support higher-dimensional data
    */
  message Scatter {
    /** @brief Output tensor dimensions
     *
     *  List of integers. Number of dimensions must
     *  match data tensor.
     */
    repeated int64 dims = 1;
    /// Dimension to scatter along
    google.protobuf.UInt64Value axis = 2;
  }

  /** @brief Gather values from specified tensor indices
    *
    *  Expects two input tensors: an @f$ N @f$-D data tensor and a 1D
    *  index vector. For 1D data:
    *  @f[
    *    y[i] = x[\text{ind}[i]]
    *  @f]
    *  If an index is out-of-range, the corresponding output is set to
    *  zero.
    *
    *  For higher-dimensional data, the layer performs a gather along
    *  one dimension. For example, with 2D data and axis=1,
    *  @f[
    *    y[i,j] = x[i,\text{ind}[j]]
    *  @f]
    *  Currently, only 1D and 2D data is supported.
    *
    *  The size of the the output tensor along the gather dimension is
    *  equal to the size of the index vector. The remaining dimensions
    *  of the output tensor are identical to the data tensor.
    *
    *  @todo Support higher-dimensional data
    */
  message Gather {
    /// Dimension to gather along
    google.protobuf.UInt64Value axis = 1;
  }

  /** @brief Sum of tensor entries over batch dimension
    *
    *  Output tensor has same shape as input tensor.
    */
  message BatchwiseReduceSum {}

  /** @brief Permute a tensor's indices ("tensor transpose")
   *
   *  @note This name was chosen to not conflict with the existing
   *        LBANN module API for "Permute", which can be implemented
   *        with some reshapes and a gather if this layer is not
   *        available. However, the C++ class name is just
   *        "PermuteLayer".
   */
  message TensorPermute {
    repeated uint32 axes = 1;
  }

  /// Deprecated
  message CategoricalRandom {
  }
  /// Deprecated
  message DiscreteRandom {
    repeated double values = 1;
    repeated int64 dims = 2;
  }

  // ---------------------------
  // Learning layers
  // ---------------------------

  /** @brief Affine transformation
   *
   *  Flattens the input tensor, multiplies with a weights matrix, and
   *  optionally applies an entry-wise bias. Following a
   *  row-vector convention:
   *    @f[ y = \text{vec}(x) W^T + b @f]
   *
   *  Two weights are required if bias is applied: the linearity and the
   *  bias. Only the linearity weights are required if bias is not
   *  applied. If weights aren't provided, the linearity weights are
   *  initialized with He normal initialization and the bias weights are
   *  initialized to zero.

   *  For flat data, this layer is similar to Keras' dense layer or
   *  PyTorch's linear operation. However, it implicitly flattens
   *  multi-dimensional data. To avoid this flattening, consider the
   *  channel-wise fully-connected layer.
   */
  message FullyConnected {
    /// Output tensor size
    int64 num_neurons = 1;
    /// Whether to apply entry-wise bias
    bool has_bias = 2;
    /// Whether to apply transpose of weights matrix
    bool transpose = 3;
  }

  /** @brief Convolution
   *
   *  Applies convolution (more precisely, cross-correlation) to input
   *  tensor. This is primarily optimized for image data in CHW
   *  format.
   *
   *  Two weights are required if bias is applied: a kernel tensor (in
   *  KCHW format) and per-channel biases. Only the kernel weights are
   *  required if bias is not applied. If weights aren't provided, the
   *  kernel weights are initialized with He normal initialization and
   *  the bias weights are initialized to zero.
   */
  message Convolution {

    /** @brief Number of spatial dimensions
     *
     *  The first data dimension is treated as the channel dimension,
     *  and all others are treated as spatial dimensions (recall that
     *  the mini-batch dimension is implicit).
     */
    int64 num_dims = 1;

    /** @brief Channel dimension of output tensor */
    int64 out_channels = 2;

    /** @brief Convolution kernel dimensions (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled.
     */
    repeated int64 kernel_size = 3;

    /** @brief Convolution strides (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled.
     */
    repeated int64 stride = 4;

    /** @brief Convolution padding (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled.
     */
    repeated int64 padding = 5;

    // Deconv: output_padding = 6;

    /** @brief Number of convolution groups
     *  @details Default: 1
     */
    google.protobuf.Int64Value groups = 7;

    /** @brief Whether to apply channel-wise bias
     *  @details Default: True
     */
    google.protobuf.BoolValue has_bias = 8;

    /** @brief Convolution dilations (vector-valued)
     *
     *  List of integers, one for each spatial
     *  dimension. Used when @c has_vectors is enabled. Defaults to
     *  dilations of 1, i.e. undilated convolution.
     */
    repeated int64 dilation = 9;

    /** @brief Special behavior with FP16 tensor cores
     *  @details Ignored for non-GPU layers.
     */
    ConvTensorOpsMode conv_tensor_op_mode = 14;

  }

  /** @brief Convolution transpose
   *
   *  This operation is the transpose of standard deep learning
   *  convolution.
   *
   *  Pedantic comments: this operation is commonly called
   *  "deconvolution" in the deep learning community, but it is not a
   *  true deconvolution. Also, the "convolution" operation commonly
   *  used in the deep learning is actually cross-correlation.
   */
  message Deconvolution {

    /// Number of spatial dimensions
    int64 num_dims = 1;

    /// Channel dimension of output tensor
    int64 out_channels = 2;

    /** @brief Convolution kernel dimensions
     */
    repeated int64 kernel_size = 3;

    /** @brief Convolution stride
     *  @brief Default: 1
     */
    repeated int64 stride = 4;
    /** @brief Convolution padding
     *  @details Default: 0
     */
    repeated int64 padding = 5;
    /** @brief Padding for output tensor
     *  @details The output tensor size is ambiguous when the
     *  convolution is strided. If this is not set, then we will
     *  output the smallest valid output tensor.
     */
    repeated int64 output_padding = 6;
    /** @brief Number of convolution groups
     *  @details Default: 1
     */
    google.protobuf.Int64Value groups = 7;
    /** @brief Whether to apply channel-wise bias
     *  @details Default: True
     */
    google.protobuf.BoolValue has_bias = 8;
    /** @brief Convolution dilation
     *  @details Default: 1
     */
    repeated int64 dilation = 9;

    /** @brief Special behavior with FP16 tensor cores
     *  @details Ignored for non-GPU layers.
     */
    ConvTensorOpsMode conv_tensor_op_mode = 10;

  }

  /** @brief Lookup table to embedding vectors.
   *
   *  Takes a scalar input, interprets it as an index, and outputs the
   *  corresponding vector. The number of embedding vectors and the
   *  size of vectors are fixed. If the index is out-of-range, then
   *  the output is a vector of zeros.
   *
   *  The embedding vectors are stored in an
   *  @f$ \text{embedding_dim} \times \text{num_embeddings} @f$
   *  weights matrix. Note that this is the transpose of the weights
   *  in the PyTorch embedding layer.
   */
  message Embedding {
    /// Size of dictionary of embeddings
    int64 num_embeddings = 1;
    /// Size of embedding vectors
    int64 embedding_dim = 2;
    /** If the padding index is set, then the corresponding embedding
     *  vector is initialized with zeros. The objective function
     *  gradient w.r.t. this embedding vector is always zero.
     */
    google.protobuf.Int64Value padding_idx = 3;
  }

  /** @brief Apply per-channel scale and bias
   *
   *  The input tensor is sliced along the first tensor dimension (the
   *  "channel" dimension, assuming image data in CHW format) and scale
   *  and bias terms are applied independently to each slice. More
   *  precisely, given input and output tensors
   *  @f$ X,Y\in\mathbb{R}^{d_1\times\cdots\times d_n} @f$
   *  and scale and bias vectors @f$ a,b\in\mathbb{R}^{d_1} @f$:
   *  @f[
   *    Y_{i,j,\cdots} = a_i X_{i,j,\cdots} + b_i
   *  @f]
   *
   *  The scale and bias vectors are fused into a single weights
   *  tensor to reduce the number of gradient allreduces during
   *  backprop. In particular, the weights tensor is a
   *  @f$ \text{num\_channels} \times 2 @f$ matrix, where the first
   *  column corresponds to scale terms and the second column to bias
   *  terms.
   */
  message ChannelwiseScaleBias {}

  /** @brief Apply entry-wise scale and bias
   *
   *  Scale and bias terms are applied independently to each tensor
   *  entry. More precisely, given input, output, scale, and bias
   *  tensors @f$ X,Y,A,B\in\mathbb{R}^{d_1\times\cdots\times d_n} @f$:
   *  @f[
   *    Y = A \circ X + B
   *  @f]
   *
   *  The scale and bias terms are fused into a single weights tensor to
   *  reduce the number of gradient allreduces during backprop. In
   *  particular, the weights tensor is a
   *  @f$ \text{size} \times 2 @f$ matrix, where the first
   *  column correspond to scale terms and the second column to bias
   *  terms.
   */
  message EntrywiseScaleBias {}

  /** @brief Apply affine transformation to tensor channels
   *
   *  The input tensor is sliced along the first tensor dimension (the
   *  "channel" dimension for image data in CHW format) and the same
   *  affine transformation is applied to each slice. Following a
   *  row-vector convention:
   *    @f[ y(i,*) = \text{vec}( x(i,*) ) W^T + b @f]
   *
   *  Two weights are required if bias is applied: the linearity and the
   *  bias. Only the linearity weights are required if bias is not
   *  applied. If weights aren't provided, the linearity weights are
   *  initialized with He normal initialization and the bias weights are
   *  initialized to zero.
   */
  message ChannelwiseFullyConnected {
    /// Output tensor dimensions, excluding the channel dimension
    repeated uint64 output_channel_dims = 1;
    /** @brief Whether to apply bias
     *  @details Default: true
     */
    google.protobuf.BoolValue bias = 2;
    /** @brief Whether to apply transpose of weights matrix
     *  @details Default: false
     */
    google.protobuf.BoolValue transpose = 3;
  }

  /** @brief Stacked gated recurrent unit
   *
   *  Expects two inputs: a 2D input sequence
   *  ( @f$ \text{sequence\_length}\times\text{input\_size} @f$ )
   *  and a 2D initial hidden state
   *  ( @f$ \text{num\_layers}times\text{hidden\_size} @f$ ).
   *
   *  Uses four weights per GRU cell: "ih\_matrix" (
   *  @f$ 3 \text{hidden\_size}\times\text{input\_size} @f$ for layer
   *  0 and @f$ 3 \text{hidden\_size}\times\text{hidden\_size} for
   *  other layers), "hh\_matrix" (
   *  @f$ 3 \text{hidden\_size}\times\text{hidden\_size} @f$ ),
   *  "ih_bias" ( @f$ 3 \text{hidden\_size} @f$ ),
   *  "hh_bias" ( @f$ 3 \text{hidden\_size} @f$ ).
   *
   *  Support is experimental and requires either cuDNN (on GPU) or
   *  oneDNN (on CPU).
   *
   *  @todo Support bidirectional RNNs
   */
  message GRU {
    /** @brief Size of each hidden state and output vector */
    uint64 hidden_size = 1;
    /** @brief Number of stacked GRU cells
     *  @details Default: 1
     */
    google.protobuf.UInt64Value num_layers = 2;
  }

  // ---------------------------
  // Image layers
  // ---------------------------

  /** @brief Resize image with bilinear interpolation
   *
   *  Expects a 3D input tensor, which is interpreted as an image in
   *  CHW format. Gradients are not propagated during backprop.
   */
  message BilinearResize {
    /** @brief Output image height */
    int64 height = 1;
    /** @brief Output image width */
    int64 width = 2;
  }

  /** @brief Rotate a image clockwise around its center
   *
   *  Expects two inputs: a 3D image tensor in CHW format and a scalar
   *  rotation angle.
   */
  message Rotation {}

  /** @brief Rotate a image clockwise around its center, then shear , then translate
   *
   *  Expects 4 inputs: a 3D image tensor in CHW format, a scalar
   *  rotation angle, a tensor for (X,Y) shear factor, a tensor
   *  for (X,Y) translate.
   */
  message CompositeImageTransformation {}

  /** @brief Cutout a square from an image
   *
   *  Expects two inputs: a 3D image tensor in CHW format and a scalar
   *  length of the cutout square.
   */
  message Cutout {}

  // ---------------------------
  // Miscellaneous layers
  // ---------------------------

  /** @brief Covariance between entries of two tensors */
  message Covariance {
    /// Use biased estimator, i.e. sample covariance
    bool biased = 1;
  }

  /** @brief Variance of tensor entries */
  message Variance {
    /// Use biased estimator, i.e. sample variance
    bool biased = 1;
  }

  /** @brief Mean values across channel dimension
   *
   *  The input tensor is sliced along the first tensor dimension (the
   *  "channel" dimension for image data in CHW format) and the mean
   *  value is computed for each slice.
   */
  message ChannelwiseMean {}

  /** @brief Position of data sample within mini-batch
   *
   *  LBANN does implicit mini-batching and data samples are usually
   *  processed independently. This layer is helpful if some
   *  mini-batch samples need to be processed differently from others.
   */
  message MiniBatchIndex {}

  /** @brief Size of current mini-batch */
  message MiniBatchSize {}

  /** @brief Get index of maximum-value tensor entry
   *
   *  Expects a 1D input tensor. If multiple entries have the same
   *  maximum value, outputs the index of the first one.
   */
  message Argmax {}

  /** @brief Get index of minimum-value tensor entry
   *
   *  Expects a 1D input tensor. If multiple entries have the same
   *  minimum value, outputs the index of the first one.
   */
  message Argmin {}

  /** @brief Convert index to a one-hot vector
   *
   *  Expects a scalar input tensor and outputs a 1D tensor.
   *  The input is interpreted as an index, and output entries are one
   *  if they correspond to that index and zero otherwise.
   *  Out-of-range indices are ignored.
   */
  message OneHot {
    /// Size of one-hot vector
    int64 size = 1;
  }

  /** @brief Softmax across channel dimension
   *
   *  The input tensor is sliced along the first tensor dimension (the
   *  "channel" dimension for image data in CHW format) and the
   *  softmax function is computed for each slice.
   */
  message ChannelwiseSoftmax {}

  /** @brief Embedding layer with distributed weights
   *

   *  This is similar to the embedding layer, which takes integer
   *  indices and returns embedding vectors from a lookup table.
   *  However, the embedding vectors are distributed between processes
   *  and one-sided inter-process communication is performed with
   *  OpenSHMEM (on CPU) or NVSHMEM (on GPU).
   *
   *  The main benefit of this model-parallel approach is to handle
   *  cases where the embedding vectors don't fit on one process. It
   *  should also have better scaling properties when the mini-batch
   *  size is very large.
   *
   *  To take advantage of sparse gradients, the distributed embedding
   *  layer provides the option to bypass the optimizer (which
   *  currently only supports dense gradients) and perform sparse SGD
   *  directly on the embedding weights. If enabled, SGD occurs during
   *  the layers "update" phase (i.e. in the virtual update_compute
   *  function). Otherwise, the layer converts sparse gradients to a
   *  dense tensor and passes it into the usual optimizer. This is a
   *  hack and will be deprecated once the optimizer class supports
   *  sparse gradients.
   *
   *  @warning This is experimental.
   *
   *  @todo Sparse SGD with optimizer class
   */
  message DistEmbedding {

    /** Size of dictionary of embeddings. */
    int64 num_embeddings = 1;
    /** Size of embedding vectors. */
    int64 embedding_dim = 2;

    /** Perform sparse SGD during backprop.
     *
     *  Bypasses optimizer class.
     */
    bool sparse_sgd = 3;
    /** SGD learning rate. */
    double learning_rate = 4;

    /** Perform a blocking barrier at the beginning of forward prop.
     *
     *  This layer performs synchronization with non-blocking barriers
     *  to ensure the correctness of asynchronous communication.
     *  However, gradient checking changes the embedding values without
     *  performing any synchronization. The quickest fix is to do a
     *  blocking barrier at the beginning of forward prop to make sure
     *  that all the embeddings are ready to be accessed.
     *
     *  @todo Think of a way to avoid this synchronization.
     */
    bool barrier_in_forward_prop = 5;

  }

  /** @brief Apply a hash function to get uniformly distributed values
   *
   *  Each input entry is hashed with MD5 and scaled to [0,1).
   *
   *  @warning Currently only supported on GPU.
   */
  message UniformHash {}

  /** @brief L2 norm of each row of a weights matrix.
   *
   *  @warning This layer is experimental and finnicky. It is intended
   *  for use with the matrix weights from a fully-connected layer, and
   *  other use-cases may have strange behavior.
   *
   *  Given a weights object, this layer computes the L2 norm for each
   *  row of the underlying matrix. Note that the internal matrix may
   *  have different dimensions than the logical weight dimensions.
   *
   *  This layer expects to have one weights object. During setup, that
   *  weights object should be initialized by another layer before this
   *  layer's setup phase. Setting a "hint layer" may be necessary to
   *  enforce this ordering.
   */
  message RowwiseWeightsNorms {}

} // message Layer

//========================================================================
// Parallel strategies for generalized layer-wise parallelism
//========================================================================

/** @brief Configuration for advanced parallelization strategies */
message ParallelStrategy {

  // ---------------------------
  // distconv
  // ---------------------------

  int64 sample_groups = 1;
  int64 sample_splits = 2;
  int64 height_groups = 3;
  int64 height_splits = 4;
  int64 width_groups = 5;
  int64 width_splits = 6;
  int64 channel_groups = 7;
  int64 channel_splits = 8;
  int64 filter_groups = 9;
  int64 filter_splits = 10;
  // For fully-connected layers.
  int64 replications = 11;
  int64 procs_per_replica = 12;
  int64 depth_groups = 13;
  int64 depth_splits = 14;

  // ---------------------------
  // Sub-grid parallelism
  // ---------------------------

  /** @brief Identifying tag for process grid
   *
   *  Tag 0 corresponds to the trainer grid. If the tag is unspecified
   *  or negative, the process grid is chosen based on heuristics.
   */
  google.protobuf.Int64Value grid_tag = 21;

  /// @todo Remove
  int64 sub_branch_tag = 15;
  /// @todo Remove
  int64 sub_branch_resource_percentage = 16;
  /// @todo Remove
  bool enable_subgraph = 17;

}

// =============================================
// Deprecated messages
// =============================================

/// Deprecated
message WeightsShape {
  repeated int64 dim = 1 [packed = true];
}
/// Deprecated
message WeightsData {
  WeightsShape shape = 5;
  string name = 1;
  int64 height = 2;
  int64 width = 3;
  repeated float data = 4 [packed=true];
  Imcomm imcomm = 55;
}
/// Deprecated
enum Imcomm {
  DEFAULT = 0; //add Layer to Imcomm callback if all_learning_layers = true in
               //the CallbackImComm
  EXCLUDE = 1; //*do not* add Layer to Imcomm callback if all_learning_layers = true in
               //the CallbackImComm
  INCLUDE = 2;  //add Layer to Imcomm callback regardless of whether all_learning_layers
                //in the CallbackImComm is set to true or false
}
